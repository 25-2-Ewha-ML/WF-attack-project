# -*- coding: utf-8 -*-
"""Feature extraction-v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Kspe4kgT6LUXdA_LNEdDS4-T5woTD4M_

"""


"""# 2. 통합 csv 파일 생성

## 1번 데이터셋 생성
"""

# ============================================
# CONFIG
# ============================================
import pickle, math, numpy as np, pandas as pd
from tqdm import tqdm

# --- Monitored / Unmonitored pickle 경로 ---
MON_PATH   = "data/raw/mon_standard.pkl"    
UNMON_PATH = "data/raw/unmon_standard10.pkl" 

URL_PER_SITE = 10     # monitored용
LAST_TS_MAX  = 80.0
PACKETS_MAX  = 10000
MAX_DT_PCTL  = 99.5   # 가장 긴 패킷 간격 컷 (percentile)

# 출력 파일 이름 
MON_OUT_CLEAN = "data/processed/monitored_features_1.csv"  
UNMON_OUT_CLEAN = "data/processed/unmonitored_features_1.csv" 


# ============================================
# 공용 유틸 함수
# ============================================
def safe_abs(x):
    return abs(float(x))

def alt_concentration_sum(timestamps):
    """
    1초 단위 packet count → counts² 합
    (기존 sum_alt_concentration_list 로직)
    """
    if len(timestamps) == 0:
        return 0.0
    secs = np.floor(np.array(timestamps, dtype=float)).astype(int)
    if secs.max() < 0:
        return 0.0
    counts = np.bincount(secs, minlength=secs.max() + 1)
    return float(np.sum(counts.astype(float) ** 2))


def compute_session_features(t, sgn, label):
    """
    하나의 세션(sample)에 대해 feature들을 계산해서 dict로 반환.
    t: np.array of timestamps (absolute)
    sgn: np.array of sign (direction: +1 outgoing, -1 incoming)
    label: 클래스 라벨 (monitored: 0~94, unmon: -1)
    """
    packets = len(t)
    in_mask  = (sgn < 0)
    out_mask = (sgn > 0)

    in_cnt  = int(np.sum(in_mask))
    out_cnt = int(np.sum(out_mask))

    # 비율
    in_frac  = in_cnt / packets if packets > 0 else 0.0
    out_frac = out_cnt / packets if packets > 0 else 0.0

    # outgoing packet index 통계
    out_idx = np.nonzero(out_mask)[0]
    out_idx_std  = float(np.std(out_idx)) if len(out_idx) > 0 else 0.0
    out_idx_mean = float(np.mean(out_idx)) if len(out_idx) > 0 else 0.0

    # alt_concentration (1초 단위 count^2 합)
    alt_conc_sum = alt_concentration_sum(t)

    # first 30 패킷 내 incoming/outgoing count
    k = min(30, packets)
    in_first30  = int(np.sum(in_mask[:k]))
    out_first30 = int(np.sum(out_mask[:k]))

    # last_ts / max_dt
    last_ts = float(t[-1]) if packets > 0 else 0.0
    max_dt  = float(np.max(np.diff(t))) if packets > 1 else 0.0

    # ============================
    # Burst 기반 feature
    # ============================
    if packets > 0:
        dir_seq = np.sign(sgn)  # +1 / -1
    else:
        dir_seq = np.array([], dtype=float)

    burst_lengths = []
    burst_dirs    = []

    if len(dir_seq) > 0:
        current_dir = dir_seq[0]
        current_len = 1
        for d in dir_seq[1:]:
            if d == current_dir:
                current_len += 1
            else:
                burst_lengths.append(current_len)
                burst_dirs.append(current_dir)
                current_dir = d
                current_len = 1
        burst_lengths.append(current_len)
        burst_dirs.append(current_dir)

    burst_lengths = np.array(burst_lengths, dtype=float) if len(burst_lengths) > 0 else np.array([], dtype=float)
    burst_dirs    = np.array(burst_dirs, dtype=float) if len(burst_dirs) > 0 else np.array([], dtype=float)

    num_bursts = len(burst_lengths)
    if num_bursts > 0:
        mean_burst_length = float(burst_lengths.mean())
        max_burst_length  = float(burst_lengths.max())
        # burst_size_std, mean_incoming_burst_length 등은 corr 결과로 제외하기로 했으므로 계산/저장 X
    else:
        mean_burst_length = 0.0
        max_burst_length  = 0.0

    # outgoing burst 평균 길이
    if len(burst_dirs) > 0:
        outgoing_burst_lengths = burst_lengths[burst_dirs > 0]
        if len(outgoing_burst_lengths) > 0:
            mean_outgoing_burst_length = float(outgoing_burst_lengths.mean())
        else:
            mean_outgoing_burst_length = 0.0
    else:
        mean_outgoing_burst_length = 0.0

    # ============================
    # IAT (inter-arrival time)
    # ============================
    if packets > 1:
        iats = np.diff(t)
        iat_mean = float(iats.mean())
        # iat_median, iat_std는 corr 결과로 제외 → 계산하지 않음
    else:
        iat_mean = 0.0

    # ============================
    # packets per second
    # ============================
    duration = t[-1] - t[0] if packets > 0 else 0.0
    if duration > 0:
        packets_per_sec = float(packets / duration)
    else:
        packets_per_sec = float(packets)

    # ============================
    # direction 변화 횟수
    # ============================
    if len(dir_seq) > 1:
        direction_changes = int(np.sum(dir_seq[1:] != dir_seq[:-1]))
    else:
        direction_changes = 0

    # ============================
    # row dict 구성
    # ============================
    row = {
        "label": label,

        # 기존 feature 중 유지할 것들
        # ( 상관계수에 따라 아래 컬럼은 포함 X:
        #  num_incoming_packets, num_incoming_packets_log1p,
        #  num_incoming_packets_fraction, std_outgoing_packet_order_idx,
        #  sum_in_out_total)
        "num_outgoing_packets": out_cnt,
        "num_outgoing_packets_fraction": out_frac,
        "mean_outgoing_packet_order_idx": out_idx_mean,
        "sum_alt_concentration_list": alt_conc_sum,
        "total_packets": packets,
        "incoming_count_first30": in_first30,
        "outgoing_count_first30": out_first30,
        "last_ts": last_ts,
        "max_dt": max_dt,

        # 새로 추가한 feature들 (corr 결과로 남기기로 한 것들)
        "num_bursts": num_bursts,
        "mean_burst_length": mean_burst_length,
        "mean_outgoing_burst_length": mean_outgoing_burst_length,
        "max_burst_length": max_burst_length,
        "iat_mean": iat_mean,
        "packets_per_sec": packets_per_sec,
        "direction_changes": direction_changes,
    }

    return row, last_ts, packets, max_dt


def build_df_from_pickle(path, is_monitored=True):
    """
    path에 있는 pickle 파일로부터 feature DataFrame 생성.
    - monitored: data[url_i] = 해당 URL의 여러 세션 (list of samples)
                 라벨은 url_i // URL_PER_SITE
    - unmonitored: data[i] = 세션 하나 (sample)
                   라벨은 모두 -1
    outlier_* 및 is_outlier는 내부에서만 사용하고 최종 CSV에는 포함 X.
    """
    print(f"\n[+] Loading dataset from {path} ...")
    with open(path, "rb") as f:
        data = pickle.load(f)

    print(f"Total entries (len(data)) = {len(data)}")

    # 1) max_dt cutoff 계산 (outlier 판단용)
    all_max_dt = []

    if is_monitored:
        # monitored: data[url_i]가 여러 sample을 가지는 구조
        for url_i in range(len(data)):
            for sample in data[url_i]:
                t = [safe_abs(c) for c in sample]
                if len(t) > 1:
                    dt = np.diff(t)
                    all_max_dt.append(np.max(dt))
                else:
                    all_max_dt.append(0.0)
    else:
        # unmonitored: data[i] 자체가 sample 하나
        for sample in data:
            t = [safe_abs(c) for c in sample]
            if len(t) > 1:
                dt = np.diff(t)
                all_max_dt.append(np.max(dt))
            else:
                all_max_dt.append(0.0)

    max_dt_cut = float(np.percentile(np.array(all_max_dt), MAX_DT_PCTL))
    print(f"[Cutoffs] LAST_TS_MAX={LAST_TS_MAX}, PACKETS_MAX={PACKETS_MAX}, "
          f"max_dt {MAX_DT_PCTL}pct={max_dt_cut:.3f}")

    # 2) feature extraction
    rows = []

    if is_monitored:
        # monitored: URL 단위 루프 + sample 루프
        for url_i in tqdm(range(len(data))):
            label = url_i // URL_PER_SITE
            for sample in data[url_i]:
                t = np.array([safe_abs(c) for c in sample], dtype=float)
                sgn = np.sign(sample)

                row, last_ts, packets, max_dt = compute_session_features(t, sgn, label)

                outlier_last_ts = last_ts > LAST_TS_MAX
                outlier_packets = packets > PACKETS_MAX
                outlier_max_dt  = max_dt > max_dt_cut
                is_outlier = outlier_last_ts or outlier_packets or outlier_max_dt

                row["outlier_last_ts"] = outlier_last_ts
                row["outlier_packets"] = outlier_packets
                row["outlier_max_dt"]  = outlier_max_dt
                row["is_outlier"]      = is_outlier

                rows.append(row)
    else:
        # unmonitored: data[i] 자체가 sample
        for sample in tqdm(data):
            t = np.array([safe_abs(c) for c in sample], dtype=float)
            sgn = np.sign(sample)

            label = -1  # unmonitored는 모두 -1
            row, last_ts, packets, max_dt = compute_session_features(t, sgn, label)

            outlier_last_ts = last_ts > LAST_TS_MAX
            outlier_packets = packets > PACKETS_MAX
            outlier_max_dt  = max_dt > max_dt_cut
            is_outlier = outlier_last_ts or outlier_packets or outlier_max_dt

            row["outlier_last_ts"] = outlier_last_ts
            row["outlier_packets"] = outlier_packets
            row["outlier_max_dt"]  = outlier_max_dt
            row["is_outlier"]      = is_outlier

            rows.append(row)

    df = pd.DataFrame(rows)
    print(f"Generated {len(df)} session rows for {df['label'].nunique()} labels")

    # 3) log1p 컬럼 추가
    log_cols = [
        "num_outgoing_packets",
        "sum_alt_concentration_list",
        "total_packets",
        "last_ts",
        "max_dt",
    ]
    for col in log_cols:
        if col in df.columns:
            df[col + "_log1p"] = np.log1p(df[col].astype(float))

    return df


# ============================================
# Monitored 데이터셋 생성
# ============================================
df_mon = build_df_from_pickle(MON_PATH, is_monitored=True)

# outlier 컬럼들 (최종 CSV에서 제거할 것)
OUTLIER_COLS = ["outlier_last_ts", "outlier_packets", "outlier_max_dt", "is_outlier"]

# clean 버전 (is_outlier == False만 사용)
df_mon_clean = df_mon[~df_mon["is_outlier"]].reset_index(drop=True)
df_mon_clean = df_mon_clean.drop(columns=OUTLIER_COLS, errors="ignore")

print("[Monitored] clean shape:", df_mon_clean.shape)

df_mon_clean.to_csv(MON_OUT_CLEAN, index=False)

print("\nSaved monitored datasets:")
print(" -", MON_OUT_CLEAN)


# ============================================
# Unmonitored 데이터셋 생성
# ============================================
df_unmon = build_df_from_pickle(UNMON_PATH, is_monitored=False)

df_unmon_full  = df_unmon.drop(columns=OUTLIER_COLS, errors="ignore")
df_unmon_clean = df_unmon[~df_unmon["is_outlier"]].reset_index(drop=True)
df_unmon_clean = df_unmon_clean.drop(columns=OUTLIER_COLS, errors="ignore")

print("[Unmonitored] clean shape:", df_unmon_clean.shape)

df_unmon_clean.to_csv(UNMON_OUT_CLEAN, index=False)

print("\nSaved unmonitored datasets:")
print(" -", UNMON_OUT_CLEAN)

"""## 2번 데이터셋 생성"""

MON_V1_PATH   = "data/processed/monitored_features_1.csv"
UNMON_V1_PATH = "data/processed/unmonitored_features_1.csv"

MON_OUTPUT_2  = "data/processed/monitored_features_2.csv"
UNMON_OUTPUT_2 = "data/processed/unmonitored_features_2.csv"

# 삭제할 noisy 컬럼들 (오타 포함)
NOISY_COLS = [
    'sum_alt_concentration_list',
    'sum_alt_concentration_list_log1p',
    'total_packets_log1p',
    'num_outgoing_packets_log1p',
    'last_ts_log1p',
    'max_dt_log1p'
]

# === Load v1 dataset ===
df_mon = pd.read_csv(MON_V1_PATH)
df_unmon = pd.read_csv(UNMON_V1_PATH)

print("Before drop shapes:")
print("Monitored :", df_mon.shape)
print("Unmonitored :", df_unmon.shape)

# === Drop noisy columns ===
df_mon_2   = df_mon.drop(columns=[c for c in NOISY_COLS if c in df_mon.columns])
df_unmon_2 = df_unmon.drop(columns=[c for c in NOISY_COLS if c in df_unmon.columns])

print("\nAfter drop shapes:")
print("Monitored :", df_mon_2.shape)
print("Unmonitored :", df_unmon_2.shape)

# === Save dataset 2 ===
df_mon_2.to_csv(MON_OUTPUT_2, index=False)
df_unmon_2.to_csv(UNMON_OUTPUT_2, index=False)

print("\nSaved:")
print(" -", MON_OUTPUT_2)
print(" -", UNMON_OUTPUT_2)

print("\n[Monitored Dataset 2 Head]")
print(df_mon_2.head())

print("\n[Unmonitored Dataset 2 Head]")
print(df_unmon_2.head())


"""## 4번 데이터셋 생성"""

MON_V1_PATH   = "data/processed/monitored_features_1.csv"
UNMON_V1_PATH = "data/processed/unmonitored_features_1.csv"

MON_OUTPUT_4  = "data/processed/monitored_features_4.csv"
UNMON_OUTPUT_4 = "data/processed/unmonitored_features_4.csv"

# 삭제할 컬럼들
NOISY_COLS = [
    'sum_alt_concentration_list',
    'sum_alt_concentration_list_log1p',
    'total_packets',
    'num_outgoing_packets',
    'last_ts',
    'max_dt'
]

# === Load v1 dataset ===
df_mon = pd.read_csv(MON_V1_PATH)
df_unmon = pd.read_csv(UNMON_V1_PATH)

print("Before drop shapes:")
print("Monitored :", df_mon.shape)
print("Unmonitored :", df_unmon.shape)

# === Drop noisy columns ===
df_mon_4   = df_mon.drop(columns=[c for c in NOISY_COLS if c in df_mon.columns])
df_unmon_4 = df_unmon.drop(columns=[c for c in NOISY_COLS if c in df_unmon.columns])

print("\nAfter drop shapes:")
print("Monitored :", df_mon_4.shape)
print("Unmonitored :", df_unmon_4.shape)

# === Save dataset 4 ===
df_mon_4.to_csv(MON_OUTPUT_4, index=False)
df_unmon_4.to_csv(UNMON_OUTPUT_4, index=False)

print("\nSaved:")
print(" -", MON_OUTPUT_4)
print(" -", UNMON_OUTPUT_4)

print("\n[Monitored Dataset 4 Head]")
print(df_mon_4.head())

print("\n[Unmonitored Dataset 4 Head]")
print(df_unmon_4.head())
